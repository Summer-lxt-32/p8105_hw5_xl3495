---
title: "p8105_hw5_xl3495"
author: "Xueting Li"
date: "2024-11-09"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
set.seed(123)
```

# Problem 1

Birthday generator function
```{r}
birthday_draw = function(n) {
  random_birthdays = sample(1:365, n, replace = TRUE)
  
  duplicates = duplicated(random_birthdays)
  
  result =  any(duplicates)
  
  return(result)
}
```

Simulations
```{r}
simulation = function(group_size, trials = 10000){
  p = map_dbl(group_size, function(n){
    
    results = replicate(trials, birthday_draw(n))
    
    return(mean(results))
  })
  
  return(p)
}

probs = simulation(2:50)

probs
```



```{r}
df = data.frame(
  group_size = 2:50,
  probability = probs
)

ggplot(df, aes(x = group_size, y = probability)) +
  geom_point(color = "blue", size = 3) +
  geom_line(color = "blue", lwd = 1) +   
  labs(
    title = "Probability of At Least Two People Sharing a Birthday",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  ) +
  scale_x_continuous(breaks = seq(2, 50, by = 2)) +  # Customize x-axis labels
  scale_y_continuous(labels = scales::percent) 
```

It turns out that as the group size goes larger and larger, the probability of at least two people sharing a birthday converges to 1. In other words, it is very likely to (almost must) have two people with the same birthday when randomly select over 50 people.

# Problem 2

```{r}
n = 30
sigma = 5
alpha= 0.05
num_datasets = 5000

simulation_norm = function(mu){
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_test = t.test(x, mu = 0) |>
    broom::tidy()

  #my column 'estimate' saves values of mu_hat
  tibble(mu = mu, estimate = t_test$estimate, p_value = t_test$p.value)
}

mu_values = c(0, 1, 2, 3, 4, 5, 6)

simulation_2 = lapply(mu_values, function(mu) {
  replicate(num_datasets, simulation_norm(mu), simplify = FALSE) |>
    bind_rows()
})


simulation_results_df = bind_rows(simulation_2)

proportion_results = simulation_results_df |>
  group_by(mu) |>
  summarise(power = mean(p_value < alpha))

ggplot(proportion_results, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(title = "Power of One-Sample t-Test for Different μ",
       x = "True Population Mean (μ)",
       y = "Power (Proportion of Null Rejected)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

From the plot, it shows the larger the effect size is, the larger the corresponding power.

```{r}
avg_estimate = simulation_results_df |>
  group_by(mu) |>
  summarise(avg_estimate = mean(estimate)) |>
  mutate(group = "All Samples")

avg_estimate_rejected = simulation_results_df |>
  filter(p_value < alpha) |>
  group_by(mu) |>
  summarise(avg_estimate_rejected = mean(estimate)) |>
  mutate(group = "Rejected Null Hypothesis")

combined_df = bind_rows(
  avg_estimate |> select(mu, avg_estimate = avg_estimate, group),
  avg_estimate_rejected |> select(mu, avg_estimate = avg_estimate_rejected, group)
)

ggplot(combined_df, aes(x = mu, y = avg_estimate, color = group, linetype = group)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  labs(title = "Average of Estimate Mean vs. True Mean", 
       x = "True Population Mean (μ)", 
       y = "Average of Estimate Mean") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

The sample averages of `$\hat{\mu}$ across tests for which the null hypothesis is rejected are not approximately equal to the true values of population mean $\mu$, and this discrepancy arises due to the selection bias introduced when only considering the samples where the null hypothesis is rejected. Because of this selection for extreme values, the average of the estimate will overestimate the true values.

# Problem 3

```{r}
homicide_df = read_csv("dataset/homicide-data.csv")
head(homicide_df)
```
This raw dataset contains `r nrow(homicide_df)` observations and `r ncol(homicide_df)` variables. Each observation describes a homicide with its reported date, victim's information (name, race, age, sex), location (city, state, latitude, longitude), and disposition.
```{r}
homicide_summary = homicide_df |>
  mutate(city_state = paste(city, state, sep = ", ")) |>
  group_by(city_state) |>
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"), na.rm = TRUE)
  )
```


```{r}
baltimore_summary = homicide_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_prop_test = prop.test(baltimore_summary$unsolved_homicides, baltimore_summary$total_homicides) |>
  broom::tidy()

baltimore_prop_test |>
  select(estimate, conf.low, conf.high) |>
  knitr::kable(digits = 3)
```
```{r}
all_city_prop_results = homicide_summary |>
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y)),
    tidy_results = map(prop_test, broom::tidy)
  ) |>
  select(city_state, tidy_results) |>
  unnest(tidy_results) |>
  select(city_state, estimate, conf.low, conf.high)

all_city_prop_results |>
  knitr::kable(digits = 3)
```


```{r}
ggplot(all_city_prop_results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  coord_flip() +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8)) +
  theme(plot.title = element_text(hjust = 0.5))
```




